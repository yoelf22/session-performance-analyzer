# Weekly App Scraping System

An automated system to scrape app galleries weekly and generate reports containing only new items discovered since the last run.

## üéØ Features

- **Multi-Site Scraping**: Supports Lovable Launched, Base44 Catalog, Replit Gallery, and Bolt.new Gallery
- **Deduplication**: Tracks items across runs to identify only new additions
- **Weekly Scheduling**: Automated runs on a weekly schedule
- **Comprehensive Reports**: Detailed reports showing new items, statistics, and trends
- **Data Persistence**: SQLite database for tracking all scraped items over time
- **Flexible Execution**: Run once immediately or schedule weekly runs

## üìÅ File Structure

```
anappaday_scrape/
‚îú‚îÄ‚îÄ weekly_scraper.py          # Main orchestrator for all scrapers
‚îú‚îÄ‚îÄ scheduler.py               # Scheduling system with daemon mode
‚îú‚îÄ‚îÄ database.py                # SQLite database for item tracking
‚îú‚îÄ‚îÄ setup_weekly_scraping.py   # Setup script for installation
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ weekly_scraping_data/      # Data directory (created automatically)
‚îÇ   ‚îú‚îÄ‚îÄ scraping_history.db    # SQLite database
‚îÇ   ‚îú‚îÄ‚îÄ *_all_*.json          # Complete scraping results
‚îÇ   ‚îú‚îÄ‚îÄ *_new_*.json          # New items only
‚îÇ   ‚îî‚îÄ‚îÄ weekly_report_*.txt   # Human-readable reports
‚îî‚îÄ‚îÄ logs/                      # Log files
```

## üöÄ Quick Start

### 1. Setup
```bash
# Run the setup script
python setup_weekly_scraping.py
```

This will:
- Install Python dependencies (playwright, beautifulsoup4, schedule)
- Install Playwright browsers
- Create necessary directories
- Test that all modules work correctly

### 2. Run Once (Testing)
```bash
# Run all scrapers immediately
python scheduler.py --mode run-once
```

### 3. Start Weekly Scheduling
```bash
# Schedule weekly runs every Monday at 9:00 AM
python scheduler.py --mode schedule --day monday --time 09:00
```

## üìä Understanding the Output

### Database Tracking
The system maintains a SQLite database (`weekly_scraping_data/scraping_history.db`) that tracks:
- All items ever scraped from each site
- First seen and last seen timestamps
- Deduplication hashes to identify unique items
- Scraping run statistics

### Generated Files

**Per-Run Files:**
- `{site}_all_{timestamp}.json` - All items found in this run
- `{site}_new_{timestamp}.json` - Only NEW items (not seen before)
- `weekly_summary_{timestamp}.json` - Summary statistics for the run
- `weekly_report_{timestamp}.txt` - Human-readable report

**Example New Items File:**
```json
{
  "scrape_timestamp": "2025-08-26 10:30:00",
  "site": "Bolt.new Gallery",
  "url": "https://bolt.new/gallery/all",
  "new_items_count": 5,
  "new_items": [
    {
      "title": "Amazing New App",
      "author": "Developer Name",
      "url": "https://example.com",
      "likes": 42,
      "first_seen": "2025-08-26 10:30:00"
    }
  ]
}
```

## üîß Configuration Options

### Scheduling Options
```bash
# Different days and times
python scheduler.py --mode schedule --day tuesday --time 14:30
python scheduler.py --mode schedule --day friday --time 18:00

# Available days: monday, tuesday, wednesday, thursday, friday, saturday, sunday
```

### Running Individual Sites
```python
from weekly_scraper import WeeklyScraper
import asyncio

async def run_single_site():
    scraper = WeeklyScraper()
    result = await scraper.run_single_scraper('bolt')  # or 'lovable', 'base44', 'replit'
    print(result)

asyncio.run(run_single_site())
```

## üìà Monitoring and Reports

### Weekly Report Example
```
üìä WEEKLY SCRAPING REPORT
==================================================
Report generated: 2025-08-26 10:30:00
Covering last 7 days

üåê SITES SUMMARY:
  ‚Ä¢ Lovable Launched
    - Total items tracked: 107
    - New this week: 3
    - Last scraped: 2025-08-26 10:25:00

  ‚Ä¢ Bolt.new Gallery  
    - Total items tracked: 252
    - New this week: 8
    - Last scraped: 2025-08-26 10:28:00

üéØ TOTAL NEW ITEMS THIS WEEK: 11

üìà RECENT SCRAPING RUNS:
  ‚Ä¢ Bolt.new Gallery - 2025-08-26 10:28:00
    Found: 252, New: 8
```

### Getting New Items Programmatically
```python
from weekly_scraper import WeeklyScraper

scraper = WeeklyScraper()

# Get new items from last 7 days
new_items = scraper.get_new_items_by_site(days=7)

for site_name, items in new_items.items():
    print(f"\n{site_name}: {len(items)} new items")
    for item in items:
        print(f"  - {item['title']}")
```

## üêß Linux Daemon Setup

For production deployment on Linux:

### 1. Systemd Service
```bash
# Copy service file (generated by setup script)
sudo cp /tmp/weekly-scraper.service /etc/systemd/system/

# Enable and start service
sudo systemctl enable weekly-scraper
sudo systemctl start weekly-scraper

# Check status
sudo systemctl status weekly-scraper

# View logs
journalctl -u weekly-scraper -f
```

### 2. Cron Job Alternative
```bash
# Edit crontab
crontab -e

# Add line for Monday 9:00 AM runs
0 9 * * 1 cd /path/to/anappaday_scrape && python scheduler.py --mode run-once >> logs/cron.log 2>&1
```

## üîç Database Schema

The SQLite database contains three main tables:

### Sites Table
```sql
CREATE TABLE sites (
    id INTEGER PRIMARY KEY,
    name TEXT UNIQUE NOT NULL,
    url TEXT NOT NULL,
    last_scraped TIMESTAMP,
    total_items INTEGER DEFAULT 0
);
```

### Items Table
```sql
CREATE TABLE items (
    id INTEGER PRIMARY KEY,
    site_id INTEGER,
    item_hash TEXT UNIQUE NOT NULL,
    title TEXT,
    url TEXT,
    author TEXT,
    description TEXT,
    image_url TEXT,
    metadata TEXT,
    first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT 1,
    FOREIGN KEY (site_id) REFERENCES sites (id)
);
```

### Scraping Runs Table
```sql
CREATE TABLE scraping_runs (
    id INTEGER PRIMARY KEY,
    site_id INTEGER,
    run_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    items_found INTEGER,
    new_items INTEGER,
    updated_items INTEGER,
    status TEXT,
    error_message TEXT,
    FOREIGN KEY (site_id) REFERENCES sites (id)
);
```

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Make sure you're in the right directory
   cd /path/to/anappaday_scrape
   
   # Activate virtual environment if using one
   source venv/bin/activate
   
   # Install dependencies
   pip install -r requirements.txt
   ```

2. **Playwright Browser Issues**
   ```bash
   # Reinstall browsers
   playwright install
   ```

3. **Permission Issues (Linux)**
   ```bash
   # Make scripts executable
   chmod +x scheduler.py weekly_scraper.py
   
   # Fix ownership
   sudo chown -R $USER:$USER weekly_scraping_data/
   ```

4. **Database Locked**
   ```bash
   # Check for running processes
   ps aux | grep python
   
   # Kill if necessary
   pkill -f "weekly_scraper"
   ```

### Logs and Debugging

- **Application logs**: `scraping.log`
- **Cron logs**: `logs/cron.log` 
- **Systemd logs**: `journalctl -u weekly-scraper`

Enable debug logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üîÑ How Deduplication Works

The system uses MD5 hashes of key fields to identify unique items:

1. **Hash Generation**: Combines title + URL + name (normalized)
2. **Storage**: Each item hash is stored in the database
3. **Comparison**: New scraping runs check hashes against existing ones
4. **New Item Detection**: Items with unseen hashes are marked as "new"

This ensures that:
- ‚úÖ Identical items aren't reported as new multiple times
- ‚úÖ Updated items (same hash) get their metadata refreshed
- ‚úÖ Only genuinely new items appear in weekly reports

## üìù Customization

### Adding New Scrapers
1. Create scraper class following the existing pattern
2. Add to `weekly_scraper.py` scrapers dictionary
3. Ensure it has either `scrape_all_apps()` or `scrape_all_projects()` method
4. Items should have `title`, `url`, and other standard fields

### Changing Schedule
Modify the schedule in `scheduler.py` or pass different parameters:
```python
# Every 3 days
schedule.every(3).days.at("10:00").do(job_function)

# Multiple times per week
schedule.every().tuesday.at("10:00").do(job_function)
schedule.every().friday.at("10:00").do(job_function)
```

### Custom Reports
Extend the `WeeklyScraper.generate_weekly_report()` method to include additional metrics or formatting.

---

## üí° Tips

- **First Run**: The initial run will mark all current items as "new" since there's no history
- **Data Growth**: Database and JSON files will grow over time; consider archiving old files
- **Error Recovery**: The system logs errors but continues with other sites if one fails
- **Testing**: Use `--mode run-once` to test changes before scheduling
- **Monitoring**: Check `scraping.log` regularly for any issues